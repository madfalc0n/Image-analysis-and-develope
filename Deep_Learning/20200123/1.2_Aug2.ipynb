{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T02:03:18.910885Z",
     "start_time": "2020-01-23T02:03:16.734235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy  import expand_dims\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  과일 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T02:39:21.002540Z",
     "start_time": "2020-01-23T02:39:20.089882Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_40 to have shape (3,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0c22a4ff25af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m#50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         validation_steps=150 // batch_size)     # 150/16   한번에 50개씩 생성\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#모델 저장 및 평가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myoung_lab\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_40 to have shape (3,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# 검증 및 테스트 이미지는 augmentation을 적용하지 않음(이미지 원본을 사용)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 이미지를 배치 단위로 불러와 줄 generator입니다.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'clean-dataset/train', \n",
    "        target_size=(150, 150), \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary') \n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'clean-dataset/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'clean-dataset/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', ##loss를 용도에 사용하는것도 중요하다.\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# steps_per_epoch는 한 세대마다 몇 번 생성기로부터 데이터를 얻을지를 나타내는 값\n",
    "# 한 세대마다 사용되는 학습데이터의 수는 steps_per_epoch * batch_size\n",
    "            \n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=600 // batch_size,    # 600/16     한번에 125개씩 생성\n",
    "        epochs=5,  #50\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=150 // batch_size)     # 150/16   한번에 50개씩 생성\n",
    "\n",
    "#모델 저장 및 평가\n",
    "model.save(\"clean_data_use_aug.h5\")\n",
    "scores = model.evaluate_generator( test_generator,       steps = 5)\n",
    "print(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T02:23:46.496465Z",
     "start_time": "2020-01-23T02:08:55.544524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.0074 - accuracy: 0.3365 - val_loss: -5.0831 - val_accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: -0.0307 - accuracy: 0.3300 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 17s 137ms/step - loss: -0.0685 - accuracy: 0.3328 - val_loss: 7.6246 - val_accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.0308 - accuracy: 0.3300 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.1762 - accuracy: 0.3345 - val_loss: -5.0831 - val_accuracy: 0.3333\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 18s 140ms/step - loss: -0.0390 - accuracy: 0.3355 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: -0.0534 - accuracy: 0.3343 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: -0.0150 - accuracy: 0.3295 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 18s 145ms/step - loss: -0.1073 - accuracy: 0.3338 - val_loss: -10.1662 - val_accuracy: 0.3333\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.0077 - accuracy: 0.3360 - val_loss: -10.1662 - val_accuracy: 0.3333\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.0690 - accuracy: 0.3335 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 17s 140ms/step - loss: -0.0229 - accuracy: 0.3384 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.0461 - accuracy: 0.3284 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: -0.0305 - accuracy: 0.3320 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 19s 150ms/step - loss: -0.0614 - accuracy: 0.3359 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: -0.1077 - accuracy: 0.3325 - val_loss: 7.6246 - val_accuracy: 0.3333\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 18s 140ms/step - loss: 0.1608 - accuracy: 0.3340 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 5.7303e-09 - accuracy: 0.3320 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 18s 142ms/step - loss: -0.0152 - accuracy: 0.3323 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.0156 - accuracy: 0.3386 - val_loss: -5.0831 - val_accuracy: 0.3333\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 18s 147ms/step - loss: -0.0228 - accuracy: 0.3289 - val_loss: -5.0831 - val_accuracy: 0.3333\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 19s 148ms/step - loss: 0.0229 - accuracy: 0.3360 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.0535 - accuracy: 0.3330 - val_loss: 5.0831 - val_accuracy: 0.3333\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: -0.0766 - accuracy: 0.3338 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 17s 140ms/step - loss: 0.0461 - accuracy: 0.3310 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.0614 - accuracy: 0.3369 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: -0.1768 - accuracy: 0.3289 - val_loss: 12.7077 - val_accuracy: 0.3333\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.1303 - accuracy: 0.3315 - val_loss: 10.1662 - val_accuracy: 0.3333\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0687 - accuracy: 0.3323 - val_loss: -7.6246 - val_accuracy: 0.3333\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: -0.0076 - accuracy: 0.3411 - val_loss: 5.0831 - val_accuracy: 0.3333\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 18s 140ms/step - loss: -0.0767 - accuracy: 0.3300 - val_loss: -7.6246 - val_accuracy: 0.3333\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.0692 - accuracy: 0.3330 - val_loss: 10.1662 - val_accuracy: 0.3333\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0228 - accuracy: 0.3345 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 18s 142ms/step - loss: -0.2219 - accuracy: 0.3338 - val_loss: 5.0831 - val_accuracy: 0.3333\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.1835 - accuracy: 0.3254 - val_loss: 10.1662 - val_accuracy: 0.3333\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: -0.0460 - accuracy: 0.3320 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 17s 138ms/step - loss: -0.0614 - accuracy: 0.3410 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.0616 - accuracy: 0.3330 - val_loss: -2.5415 - val_accuracy: 0.3333\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: -0.0918 - accuracy: 0.3349 - val_loss: -7.6246 - val_accuracy: 0.3333\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.1688 - accuracy: 0.3340 - val_loss: -7.6246 - val_accuracy: 0.3333\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 17s 138ms/step - loss: -0.1686 - accuracy: 0.3311 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 18s 140ms/step - loss: 0.0998 - accuracy: 0.3369 - val_loss: 7.6246 - val_accuracy: 0.3333\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: -0.0310 - accuracy: 0.3325 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 17s 138ms/step - loss: -0.0689 - accuracy: 0.3338 - val_loss: 5.0831 - val_accuracy: 0.3333\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 17s 140ms/step - loss: 0.0918 - accuracy: 0.3315 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 18s 142ms/step - loss: -0.0998 - accuracy: 0.3315 - val_loss: 2.5415 - val_accuracy: 0.3333\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.0534 - accuracy: 0.3335 - val_loss: -5.0831 - val_accuracy: 0.3333\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.1224 - accuracy: 0.3359 - val_loss: 0.0000e+00 - val_accuracy: 0.3333\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.0232 - accuracy: 0.3350 - val_loss: 5.0831 - val_accuracy: 0.3333\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 18s 141ms/step - loss: -2.4735e-04 - accuracy: 0.3262 - val_loss: 7.6246 - val_accuracy: 0.3333\n",
      "0.2874999940395355\n"
     ]
    }
   ],
   "source": [
    "# augmentation 없이  학습\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255 )\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 이미지를 배치 단위로 불러와 줄 generator입니다.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'clean-dataset/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # 모든 이미지의 크기가 150x150로 조정됩니다.\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # binary_crossentropy 손실 함수를 사용하므로 binary 형태로 라벨을 불러와야 합니다.\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'clean-dataset/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'clean-dataset/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "#네트워크 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50, # 50\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "\n",
    "#모델 저장\n",
    "model.save(\"clean_data_without_aug.h5\")\n",
    "scores = model.evaluate_generator( test_generator,       steps = 5)\n",
    "print(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
