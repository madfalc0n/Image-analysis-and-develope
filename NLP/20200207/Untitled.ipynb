{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer(CountVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    Equivalent to :class:`CountVectorizer` followed by\n",
    "    :class:`TfidfTransformer`.\n",
    "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str {'filename', 'file', 'content'}\n",
    "        If 'filename', the sequence passed as an argument to fit is\n",
    "        expected to be a list of filenames that need reading to fetch\n",
    "        the raw content to analyze.\n",
    "        If 'file', the sequence items must have a 'read' method (file-like\n",
    "        object) that is called to fetch the bytes in memory.\n",
    "        Otherwise the input is expected to be a sequence of items that\n",
    "        can be of type string or byte.\n",
    "    encoding : str, default='utf-8'\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "    decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
    "        Instruction on what to do if a byte sequence is given to analyze that\n",
    "        contains characters not of the given `encoding`. By default, it is\n",
    "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "        values are 'ignore' and 'replace'.\n",
    "    strip_accents : {'ascii', 'unicode', None} (default=None)\n",
    "        Remove accents and perform other character normalization\n",
    "        during the preprocessing step.\n",
    "        'ascii' is a fast method that only works on characters that have\n",
    "        an direct ASCII mapping.\n",
    "        'unicode' is a slightly slower method that works on any characters.\n",
    "        None (default) does nothing.\n",
    "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "        :func:`unicodedata.normalize`.\n",
    "    lowercase : bool (default=True)\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "    preprocessor : callable or None (default=None)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "        Only applies if ``analyzer is not callable``.\n",
    "    tokenizer : callable or None (default=None)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "    analyzer : str, {'word', 'char', 'char_wb'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "        Option 'char_wb' creates character n-grams only from text inside\n",
    "        word boundaries; n-grams at the edges of words are padded with space.\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "        .. versionchanged:: 0.21\n",
    "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "        first read from the file and then passed to the given callable\n",
    "        analyzer.\n",
    "    stop_words : str {'english'}, list, or None (default=None)\n",
    "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "        list is returned. 'english' is currently the only supported string\n",
    "        value.\n",
    "        There are several known issues with 'english' and you should\n",
    "        consider an alternative (see :ref:`stop_words`).\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "    token_pattern : str\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
    "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
    "        only bigrams.\n",
    "        Only applies if ``analyzer is not callable``.\n",
    "    max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "    min_df : float in range [0.0, 1.0] or int (default=1)\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly lower than the given threshold. This value is also\n",
    "        called cut-off in the literature.\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "    max_features : int or None (default=None)\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "    vocabulary : Mapping or iterable, optional (default=None)\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents.\n",
    "    binary : bool (default=False)\n",
    "        If True, all non-zero term counts are set to 1. This does not mean\n",
    "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "    dtype : type, optional (default=float64)\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "    norm : 'l1', 'l2' or None, optional (default='l2')\n",
    "        Each output row will have unit norm, either:\n",
    "        * 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "        similarity between two vectors is their dot product when l2 norm has\n",
    "        been applied.\n",
    "        * 'l1': Sum of absolute values of vector elements is 1.\n",
    "        See :func:`preprocessing.normalize`.\n",
    "    use_idf : bool (default=True)\n",
    "        Enable inverse-document-frequency reweighting.\n",
    "    smooth_idf : bool (default=True)\n",
    "        Smooth idf weights by adding one to document frequencies, as if an\n",
    "        extra document was seen containing every term in the collection\n",
    "        exactly once. Prevents zero divisions.\n",
    "    sublinear_tf : bool (default=False)\n",
    "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "    fixed_vocabulary_: bool\n",
    "        True if a fixed vocabulary of term to indices mapping\n",
    "        is provided by the user\n",
    "    idf_ : array, shape (n_features)\n",
    "        The inverse document frequency (IDF) vector; only defined\n",
    "        if ``use_idf`` is True.\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "        This is only available if no vocabulary was given.\n",
    "    See Also\n",
    "    --------\n",
    "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
    "        matrix of counts.\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    >>> corpus = [\n",
    "    ...     'This is the first document.',\n",
    "    ...     'This document is the second document.',\n",
    "    ...     'And this is the third one.',\n",
    "    ...     'Is this the first document?',\n",
    "    ... ]\n",
    "    >>> vectorizer = TfidfVectorizer()\n",
    "    >>> X = vectorizer.fit_transform(corpus)\n",
    "    >>> print(vectorizer.get_feature_names())\n",
    "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "    >>> print(X.shape)\n",
    "    (4, 9)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "\n",
    "        super().__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n",
    "                                       smooth_idf=smooth_idf,\n",
    "                                       sublinear_tf=sublinear_tf)\n",
    "\n",
    "    # Broadcast the TF-IDF parameters to the underlying transformer instance\n",
    "    # for easy grid search and repr\n",
    "\n",
    "    @property\n",
    "    def norm(self):\n",
    "        return self._tfidf.norm\n",
    "\n",
    "    @norm.setter\n",
    "    def norm(self, value):\n",
    "        self._tfidf.norm = value\n",
    "\n",
    "    @property\n",
    "    def use_idf(self):\n",
    "        return self._tfidf.use_idf\n",
    "\n",
    "    @use_idf.setter\n",
    "    def use_idf(self, value):\n",
    "        self._tfidf.use_idf = value\n",
    "\n",
    "    @property\n",
    "    def smooth_idf(self):\n",
    "        return self._tfidf.smooth_idf\n",
    "\n",
    "    @smooth_idf.setter\n",
    "    def smooth_idf(self, value):\n",
    "        self._tfidf.smooth_idf = value\n",
    "\n",
    "    @property\n",
    "    def sublinear_tf(self):\n",
    "        return self._tfidf.sublinear_tf\n",
    "\n",
    "    @sublinear_tf.setter\n",
    "    def sublinear_tf(self, value):\n",
    "        self._tfidf.sublinear_tf = value\n",
    "\n",
    "    @property\n",
    "    def idf_(self):\n",
    "        return self._tfidf.idf_\n",
    "\n",
    "    @idf_.setter\n",
    "    def idf_(self, value):\n",
    "        self._validate_vocabulary()\n",
    "        if hasattr(self, 'vocabulary_'):\n",
    "            if len(self.vocabulary_) != len(value):\n",
    "                raise ValueError(\"idf length = %d must be equal \"\n",
    "                                 \"to vocabulary size = %d\" %\n",
    "                                 (len(value), len(self.vocabulary)))\n",
    "        self._tfidf.idf_ = value\n",
    "\n",
    "    def _check_params(self):\n",
    "        if self.dtype not in FLOAT_DTYPES:\n",
    "            warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n",
    "                          \"be converted to np.float64.\"\n",
    "                          .format(FLOAT_DTYPES, self.dtype),\n",
    "                          UserWarning)\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf from training set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        y : None\n",
    "            This parameter is not needed to compute tfidf.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted vectorizer.\n",
    "        \"\"\"\n",
    "        self._check_params()\n",
    "        self._warn_for_unused_params()\n",
    "        X = super().fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf, return term-document matrix.\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        y : None\n",
    "            This parameter is ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        self._check_params()\n",
    "        X = super().fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        # X is already a transformed view of raw_documents so\n",
    "        # we set copy to False\n",
    "        return self._tfidf.transform(X, copy=False)\n",
    "\n",
    "    def transform(self, raw_documents, copy=\"deprecated\"):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "        Uses the vocabulary and document frequencies (df) learned by fit (or\n",
    "        fit_transform).\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        copy : bool, default True\n",
    "            Whether to copy X and operate on the copy or perform in-place\n",
    "            operations.\n",
    "            .. deprecated:: 0.22\n",
    "               The `copy` parameter is unused and was deprecated in version\n",
    "               0.22 and will be removed in 0.24. This parameter will be\n",
    "               ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, msg='The TF-IDF vectorizer is not fitted')\n",
    "\n",
    "        # FIXME Remove copy parameter support in 0.24\n",
    "        if copy != \"deprecated\":\n",
    "            msg = (\"'copy' param is unused and has been deprecated since \"\n",
    "                   \"version 0.22. Backward compatibility for 'copy' will \"\n",
    "                   \"be removed in 0.24.\")\n",
    "            warnings.warn(msg, FutureWarning)\n",
    "        X = super().transform(raw_documents)\n",
    "        return self._tfidf.transform(X, copy=False)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'X_types': ['string'], '_skip_test': True}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
