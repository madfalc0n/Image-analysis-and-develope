# 딥러닝

> 1. 머신러닝은 알고리즘을 이용해 데이터를 분석하고 학습하며, 학습한 내용을 기반으로 판단이나 예측을 하였으나 딥러닝은 인간의 뇌처럼 여러 층을 가진 **인공신경망을 이용하여 머신러닝을 수행하는 것.**
> 2. 기본 층을 겹겹이 쌓아 올려 구성한 신경망(neural network)이라는 모델을 사용하여 표현 층을 학습
> 3. 층 기반 표현 학습(layered representations learning) 또는 계층적 표현 학습(hierarchical representations learning)
> 4. 일반적으로 개와 고양이 구별을 예시로 머신러닝 같은경우 동물에 대한 형태(귀,눈,입,수염 등)를 미리 컴퓨터에게 전달, 딥러닝은 개,고양이 사진 자체를 컴퓨터가 학습하도록 하는 것



### Tensorflow

> 1. 구글이 공개한 대규모 숫자 계산을 해주는  머신러닝 및 딥러닝 전문 라이브러리 
>
> 2. Tensor는 다차원 행렬 계산을 의미
>
> 3. 상업적인 용도로 사용할 수 있는 오픈소스(Apache 2.0)
>
> 4. C++로 만들어진 라이브러리
>
> 5. 파이썬을 사용해서 호출할 때 오버헤드가 거의 없는 구조로 설계
>
> 6. 이미지 처리와 음향 처리 등을 할 때는 추가적으로 이미지 처리에 특화된 OpenCV 라이브러리등과 함께 사용
> 7. pip install tensorflow==1.15



### Keras

> 1. 머신러닝 라이브러리 Theano와 TensorFlow를 래핑한 라이브러리
>
> 2. Sequential로 딥러닝의 각 층을 add()로 추가
>
> 3. 활성화 함수, Dropout 등 add()로 간단하게 추가
>
> 4. compile()로 모델 구축
>
> 5. loss 로 최적화 함수 지정
>
> 6. fit() 로 모델에 데이터를 학습시킴
>
> 7. Keras로 머신러닝을 수행할 때 Numpy 배열 데이터를 전달해야 한다
> 8. pip install keras



### 퍼셉트론(perceptron)

> 1. 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위
>
> 2. 뉴런과 뉴런이 서로 새로운 연결을 만들기도 하고 필요에 따라 위치를 바꾸는 것처럼, 여러 층의 퍼셉트론을 서로 연결시키고 복잡하게 조합하여 주어진 입력 값에 대한 판단을 하게 하는 것

<img src="images/Scheme-of-a-perceptron-A-nonlinear-activation-function-BULLET-is-applied-to-the.png" alt="Scheme-of-a-perceptron-A-nonlinear-activation-function-BULLET-is-applied-to-the" style="zoom:50%;" />

> Sheme of a perceptron (출처 : D Álvarez, 2017)

- 가중합 - 입력 값(x)과 가중치(w)의 곱을 모두 더한 값에 바이어스(b)를 더한 값

  - y = wx + b (w는 가중치,  b는 바이어스)
  
  


### 오차 역전파(back propagation)

>다층 퍼셉트론에서의 최적화 과정으로 신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정함 
>
>임의의 가중치를 선언하고 최소 제곱법을 이용해 오차를 구한 뒤 이 오차가 최소인 지점으로 계속해서 조금씩 이동시킴.
>
>오차가 최소가 되는 점(미분했을 때 기울기가 0이 되는 지점)이 우리가 알고자 하는 답이다.
>
>각 층을 지날 떄마다 층별로 가중치를 수정해야 함
>
>새 가중치는 현 가중치에서 가중치에 대한 기울기를 뺀 값

#### 과정

1. 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것

2. 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법

3. 가중치를 수정하려면 미분 값, 즉 기울기가 필요





#### 문제점

1. 활성화 함수 시그모이드는 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실 문제가 발생한다.
2. 시그모이드를 미분하면 최대치가 0.3이며 1보다 작으므로 계속 곱하다 보면 0에 가까워진다.
3. 즉 시그모이드는 층을 거쳐 갈수록 기울기가 사라져 가중치를 수정하기 어려워 진다.

<img src="images/sigmoid_1.jpg" alt="sigmoid_1" style="zoom:50%;" />

> 출처: 모두의 딥러닝

#### 대안

1. 다른 함수로 대체

<img src="images/sigmoid_2.jpg" alt="sigmoid_2" style="zoom: 67%;" />

> 출처: 모두의 딥러닝

 - 하이퍼볼릭 탄젠트(tanh) - 시그모이드 함수의 범위를 -1에서 1로 확장, 미분한 값의 범위가 함께 확장 되는 효과를 가져왔다. 여전히 1보다 작은 값이 존재하므로 기울기 소실 문제는 사라지지 않는다.


 - 렐루(ReLU) - x가 0보다 작을 때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를 그대로 사용하는 방법 x가 0보다 크기만 하면 미분 값이 1이 된다. 따라서 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있다.



## 참조

- 참조1(퍼셉트론) : [http://www.incodom.kr/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0](http://www.incodom.kr/기계학습/퍼셉트론)