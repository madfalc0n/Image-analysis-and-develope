# 딥러닝 모델을 이용한 전립선 암 검체 프로그램

## 1. 선정배경

아무래도 예전부터 TV를 통해 주변에서 심정지나 여러 사유로 인해 쓰러지는 사례를 한번씩 접하였다. 실제로 기사에서는 서울시 심정지환자 소생률이 전체 중 10.4% 밖에 되지 않는다고 한다(이 수치도 낮지만 기사를 보았는데 이 정도면 선진국 수준에 근접한다고 한다).  

<img src="images/README/image-20200313231821078.png" alt="image-20200313231821078" style="zoom:80%;" />

> 참고주소 : [뉴시스](https://www.msn.com/ko-kr/news/national/%EC%84%9C%EC%9A%B8%EC%8B%9C-%EC%8B%AC%EC%A0%95%EC%A7%80%ED%99%98%EC%9E%90-%EC%86%8C%EC%83%9D%EB%A5%A0-104%E2%80%A6%EC%84%A0%EC%A7%84%EA%B5%AD-%EC%88%98%EC%A4%80-%EA%B7%BC%EC%A0%91/ar-AACv6sv)

<img src="images/README/image-20200313232027763.png" alt="image-20200313232027763" style="zoom:80%;" />

또한 시대가 갈수록 CCTV 현황도 늘고 있다. 다만 문제점은 이것을 관리하는 관리자들은 한계가 있다는 것.

그래서 나는 이러한 제한적인 상황을 고려하여 소생률을 높이고자 실제 환자발생시 객체 인식 기술을 이용해 컴퓨터가 발견하고 이를 알려주는 시스템을 만들어보고자  하였다.





## 2. 프로젝트 목표

목표는 간단하게 말하면 CCTV를 통해 쓰러진 사람을 찾고 이를 인식하여 관제실에 알려주는 것을 목표로 정하였다.





## 3. 프로젝트 범위

구현 범위는 크게 3가지로 정하였다

1. 객체 인식 알고리즘을 통해 사람인지 사물인지 인식
   
   - `YOLO v3`를 이용
   
     <img src="images/README/image-20200314021936664.png" alt="image-20200314021936664" style="zoom:80%;" />
2. 사람일 경우 그 사람의 포즈를 통해 응급상황인지를 판단

   - `Pose Estimation`을 이용해 쓰러져있는 포즈를 확인하여 응급상황인지를 판단
   - `Pose Estimation`을 이용해 관절마다 정해져있는 키포인트의 위치를 이용하여 수직에 가까울 경우 정상으로 판단, 수평에 가까울 경우 쓰러진 사람으로 간주

   <img src="images/README/image-20200314021851467.png" alt="image-20200314021851467" style="zoom:80%;" />

3. 응급상황일 경우 관제실에 알람 메시지 전송

   - `웹(장고)` 또는 `챗봇(카카오)`을 이용하여 관제실에 알림메시지 전송

     <img src="images/README/image-20200314021951195.png" alt="image-20200314021951195" style="zoom:80%;" />

구현 범위를 위 대로 정할 경우 몇가지 문제를 생각했었다. 

### 예상되는 문제점

#### 실시간으로 처리할 수 없는 알고리즘

첫번째로 응급상황이 발생하는 즉시 관리자에게 신속하게 알려줘야 하므로 **실시간**으로 객체를 인식하고 응급상황인지를 빠른시간내에 처리를 해주어야 한다는 점이다. 사람을 인식하고 쓰러졌는지 판단하는데 걸리는 시간은 한 프레임당 대략 0.03초 정도 내로 이루어 져야 한다(일반적으로 보통 초당 30 프레임이라고 하니 즉 한 프레임당 0.03초 내로 처리해야한다는 말).

실제로 나는 객체 인식과 포즈 추정 알고리즘을 구현하여 직접 돌려보고 걸리는 시간을 측정해보았는데 객체 인식(YOLO v3)의 경우 CPU(AMD FX-8300, 인텔 i5급 성능)로 처리할 경우 1~3초, GPU(Tesla T4)는  0.1 ~ 0.3 초정도 걸렸고 포즈 추정(OPENPOSE)의 경우 CPU로만 처리했을때 10초 정도 걸렸다. 포즈 추정에서 사람의 키포인트를 모두 추정 후 키포인트간의 상관관계를 분석하여 포즈를 추정하는 방식인 `bottom-up`방식을 통해 구현했다면 따로 detection 과정을 거치지 않게 되어 빨라질거란 얘기가 있지만 따로 구성환경이 좋지못해 적용해보지 못하였다.

#### 쓰러진 사람은 사람인가 사물인가(+학습 데이터의 부족)?

일반적인 사람에 대한 이미지는 많으나 실제 길에 누워있거나 쓰러진 사람들에 대한 이미지 셋은 인터넷에서 찾기가 어려웠다. 실제 객체 인식 오픈소스 중 하나인 `Darknet`을 이용해 학습된 weight로 쓰러진 사람에 대해 인식을 시도하였으나 시도하지 못했는데 현재 공공장소에 설치된 CCTV들의 저급한 화질로 인해 객체를 구분하지 못하는 것으로 생각된다(+쓰러진사람에 대한 학습). 

<img src="images/README/image-20200314020916377.png" alt="image-20200314020916377" style="zoom:80%;" />

#### 알람메시지 서비스 관련

우선 관리자들이 응급상황을 편리하게 볼수 있도록 카카오챗봇을 선정하였으나 푸시메시지 발송관련해서 유료서비스로 제공한다는 것을 보게되었다. 그래서 문자SMS를 통해 알람을 전달하기로 진행했고 네이버 클라우드 플랫폼에서 제공하는 SMS API를 사용하게 되었다.



#### 결론

아무래도 실시간으로 처리해야 하다보니 알고리즘이 많이 가벼워야 하기에 "객체 인식과 포즈 추정 둘다 해야 할까?" 라는 생각을 하게 되었다. 한참 생각하다보니  `쓰러진사람`을 하나의 클래스로 정한다면 따로 포즈 추정없이 객체 인식만 이용하면 될 것 같다는 생각을 하였다. 

이렇게 된다면 나는 쓰러진 사람들에 대한 이미지데이터를 학습한다면 충분히 실시간으로도 구현이 가능할 것 같다는 생각이 들었고 이미지 데이터셋을 찾아보았다. 다행스럽게도 현재 우리나라 정부에서 운영하는 [AI 허브](http://www.aihub.or.kr/)에서 이상행동 CCTV 영상 데이터를 제공하고 있었다(회원가입후 데이터 사용 승인 받아야 함).

나는 AI 허브에서 제공하는 쓰러진 사람들에 대한 데이터를 학습하여 `쓰러진사람`이라는 하나의 클래스를 만들고  객체인식 알고리즘(YOLOv3)을 통해 실시간으로 감시하는 기능을 구현하기로 정하였다(웹은 아직 정리가 안되서 나중에 쓸 예정).

정리하면 다음과 같다.

1. **CCTV에서 송신되는 영상을 Flask 웹 서버에서 수신**
2. **수신 받은 영상에서 응급환자가 발생했는지 딥러닝 모델(YOLO v3)를 이용하여 실시간으로 감시**
3. **실시간 감시 중 응급환자가 발생한 경우 SMS API를 통해 관리자에게 해당 내용 전파 **





## 4. 프로젝트 진행상황

### 활동 별 계획표

| 순번 | 활동명                                       | 담당자 | 선행조건 (시작일) |   종료일   | 주요 활동 내용 |
| :--: | -------------------------------------------- | :----: | :---------------: | :--------: | -------------- |
|  1   | 관련 동향 및 기술 조사, 프로젝트 설계        | 김명환 |    2020.03.02     | 2020.03.05 |                |
|  2   | 학습 데이터 수집                             | 김명환 |    2020.03.06     | 2020.03.07 |                |
|  3   | 모델 설정 및 학습                            | 김명환 |    2020.03.08     | 2020.03.10 |                |
|  5   | 서비스 배포방식 검토                         | 김명환 |    2020.03.11     | 2020.03.11 |                |
|  6   | 서비스 배포방식 검토2                        | 김명환 |    2020.04.08     | 2020.04.10 |                |
|  7   | 웹 서비스 개발                               | 김명환 |    2020.04.13     | 2020.04.15 |                |
|  8   | 객체 인식 단위 테스트 진행 (1차 단위 테스트) | 김명환 |    2020.04.16     | 2020.04.16 |                |
|  9   | 웹 서비스 단위 테스트 진행 (2차 단위테스트)  | 김명환 |    2020.04.17     | 2020.04.17 |                |
|  10  | 최종 통합 테스트 진행                        | 김명환 |    2020.04.20     | 2020.04.24 |                |
|  11  | 서비스 배포(발표)                            | 김명환 |    2020.04.27     | 2020.04.28 |                |

### 활동 별 차트

<img src="images/README/image-20200606095342359.png" alt="image-20200606095342359" />




### 일자 별 상세내용

- 2020년 03월 01일 ~ 2020년 03월 08일
  
  - 객체 인식과 포즈추정에 대한 조사 및 알고리즘 구현,테스트 진행
  - GPU 사용을 위한 AWS 학습
  - AI 허브에서 데이터 사용 승인 허가 후 데이터 전처리 진행
    - 데이터는 동영상으로 제공되었으며 영상속에서 쓰러지는 상황에 대해 캡쳐 후 라벨링(bounding box) 처리
  - 해당 홈페이지가 불안전하여 내부영상에 대해서만 전처리하기로 진행(외부 데이터는 다운로드 도중 끊기는 현상으로 인해 데이터가 부족하여 사용하지 않음)
  
- 2020년 03월 09일
  - 전처리한 데이터와 Darknet을 이용하여 학습 진행
    - AWS에서 제공하는 GPU를 이용하여, epoch 2000까지 학습진행(단일 클래스로 `[클래스수]*2000`만큼 진행)
    - 테스트한 결과 쓰러진 상황이 아님에도 일반적인 상황에서도 쓰러진 상황으로 간주하여 과적합이 발생
      - 데이터 전처리 시 쓰러지는 사람 1명만 처리하였기 때문에 다양한 환경을 학습하지 못한 것으로 판단 

- 2020년 03월 10일

  - 다양한 사람들에 대한 데이터를 전처리하여 처음부터 학습 진행

    - epoch 1800 정도 진행 후 테스트 결과 쓰러진 상황 발생시 `쓰러진사람`에 대한 객체를 인식하는데 성공 함(학습시 다른 사람들에 대한 데이터를 이용)

      <img src="images/README/image-20200314025214861.png" alt="image-20200314025214861" style="zoom:80%;" />

      <img src="images/README/image-20200314025714514.png" alt="image-20200314025714514" style="zoom:80%;" />

- 2020년 03월 11일
  
  - `쓰러진사람` 발견 후 어떻게 관리자에게 정보를 제공할 지에 대해 구상
    - 웹캠+카카오 챗봇
    - 웹캠+웹 서버
    - 영상+웹 서버
    - 영상+ 카카오 챗봇
    - 영상+웹 서버+카카오 챗봇

- 2020년 04월 08일
  
  - 학습 데이터가 한정적이므로 실시간 웹캠을 통해 서비스를 시현하기에는 무리가 있다고 판단하여 기존 학습된 데이터와 유사한 실내 영상을 이용하여 웹 서버에서 응급상황 발생여부를 확인하고 관리자에게 알람을 제공하는 과정을 시연할 예정(영상 + 웹 서버 + SMS?)
  
- 2020년 04월 09일
  
  - 웹 서비스를 구현하기 위한 기술 조사를 진행하였으며 대상으로는 장고와 플라스크를 조사하였다. 웹 서비스는 원본영상 송출 및 객체인식 후 영상 송출 2가지를 제공할 예정이며 실시간 처리를 위해 병렬처리가 필요하다고 판단했다. 

- 2020년 04월 10일
  - 병렬처리를 구현하기 위해 플라스크 웹 서버를 구축하기로 정하였다. 병렬처리를 위해 멀티프로세싱에 대한 방법과 실시간 영상 송출 방법에 대해 학습하였다. 
  - 관리자에게 응급상황에 대한 신속한 알람을 제공하기 위해 간편하게 사용할 수 있는 NCP(네이버클라우드)의 SMS API를 이용하기로 하였다. 이유는 월별로 50건 무료이므로 따로 비용 걱정이 없었기 때문이었다. API를 사용하기 위한 코드예시를 보고 어떻게 사용할지 생각해보았다.
  
- 2020년 04월 13일
  
  - 멀티프로세싱에 대한 테스트를 진행하였음 기본적인 코드에 대한 이해를 위한 개념숙지
  - 플라스크 웹 영상 송신에 대한 자료조사(yield 와 컨텐츠 타입헤더에 대한 숙지)
  
- 2020년 04월 14일

  - 멀티프로세싱 테스트 진행

- 2020년 04월 16일
  - 멀티프로세싱 테스트 진행, 프로세스 별로 나뉘어 영상 송출 및 객체 인식을 따로 수행하도록 처리
  - 큐를 활용해 각 프로세스 간 데이터를 주고받도록 구현

- 2020년 04월 20일
  - 멘토링 수업 후 응급환자 인식 방법에 대해 재검토 진행, 데이터셋이 모두 크로마키 배경으로 이루어져 있어 실제 모델을 적용할 때는 실외에서 객체인식을 못할 것이라는 답변을 받음(실제로 인식이 안되었음)
  - 포즈 추정에 대한 방법을 검토하여 `OPENPOSE`에 대한 기술조사 진행
- 2020년 04월 21일
  - OPENPOSE 구현 후 사람에 대한 포즈 추정 테스트 진행
  - 카메라의 각도와 해상도에 따라 사람의 키포인트(관절)을 잘 인식하였으나 쓰러진 경우 제대로 인식이 되지 않는 현상을 발생
- 2020년 04월 22일
  - 포즈 추정활용에 대한 인식방법을 재검토하여 수직방향에서 카메라를 통해 사람이 쓰러졌을 경우 포즈 추정하도록 구성, 즉 평소 상황에 포즈 인식이 안되는 방법을 활용하여 쓰러졌을 경우 인식되는 관절의 갯수를 통해 쓰러짐을 판단하도록 구현
  - 웹 서버를 구현하여 실제 영상 송출 및 쓰러진 사람 발견에 대한 모니터링 진행
  - 실시간으로 문자가 전송되도록 NCP SMS 연동을 위한 코드 작성 및 테스트 진행 
- 2020년 04월 23일
  
  - 응급환자 발생에 대한 인식속도를 개선하기 위한 기술조사 진행



## 5. 최종정리 및 프로젝트 결과

### 개발환경에 대한 소개

내가  사용한 시스템들의 대해 소개하자면 다음과 같다.

- OS: Ubuntu 16.04.6 LTS(AWS EC2)
- 가상환경: Anaconda 4.8.2
- 개발환경: python 3.6.10, Opencv-python 4.1.2.30, Flask 1.1.1, YOLO v3
- 그 외 API: Naver Cloud Platform Simple & Easy Notification Service(SMS)

<img src="images/README/image-20200511103229610.png" alt="image-20200511103229610" style="zoom:80%;" />

### 객체 인식 알고리즘 YOLO v3 

> 논문자료 : [YOLO v3](https://pjreddie.com/media/files/papers/YOLOv3.pdf) 

YOLO(You Only Live Once), 이미지를 한 번 보는 것만으로 객체의 종류와 위치를 추측하는 딥러닝 기반의 알고리즘이다. Darknet-53(53개의 Convolutional layer)의 네트워크를 기반으로 다층 신경망 구조로 되어있고 공간적으로 분리된 bounding box 들과 class에 대한 확률과 연관된 regression 문제로 재정의한 모델이다. 3개의 출력층으로 구성되어 있으며 서로 다른 scale을 통해 bounding box를 검출한다.

<img src="images/README/image-20200511103421911.png" alt="image-20200511103421911" style="zoom:80%;" />

> 출처 : [외국사이트](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)

각 출력층에서 프레임의 픽셀 별로 커널(bounding box에 대한 속성)을 가지고 있으며 커널 사이즈는 다음과 같다.

```markdown
1 x 1 x (B X ( 5 + C )), B는 bounding box의 수(YOLO v3는 3), C는 클래스의 수
```

YOLO v3에서 default 클래스 수는 총 80개로 커널 사이즈는 1 x 1 x 255 임 

<img src="images/README/image-20200511103807836.png" alt="image-20200511103807836" style="zoom:80%;" />

각 출력층에서 픽셀에 대한 클래스의 확률을 계산하여 bounding box 검출하고 결과를 종합하여 하나의 출력값을 도출한다.

<img src="images/README/image-20200511103842978.png" alt="image-20200511103842978" style="zoom:80%;" />

오픈소스기반 뉴럴넷 프레임워크인 Darknet을 이용하여 모델 학습을 진행하였고 데이터는 AI 허브에서 제공하는 이상행동 CCTV 영상 데이터를 사용하였다. 파이썬에서 제공하는 Augmentation 라이브러리인 **Imgaug**를 사용하여 부족한 데이터를 증폭시켜 총 2144개의 데이터를 사용했다.(Train image set : 1716장, Test image set : 428장)

[라벨링 툴](https://github.com/tzutalin/labelImg)을 이용해 쓰러진 사람에 대한 bounding box를 지정하여 약 2800번의 학습 결과 Loss가 대략 0.2로 수렴하는것을 확인했다.

<img src="images/README/image-20200511104119358.png" alt="image-20200511104119358" style="zoom:80%;" />

### 시스템 동작 절차

동작 절차는 크게 3가지로 나뉜다.

1. CCTV에서 송신되는 영상을 Flask 웹 서버에서 수신

2. 수신 받은 영상에서 응급환자가 발생했는지 딥러닝 모델(YOLO v3)를 이용하여 실시간으로 감시

3. 실시간 감시 중 응급환자가 발생한 경우 SMS API를 통해 관리자에게 해당 내용 전파 

<img src="images/README/image-20200511104312019.png" alt="image-20200511104312019" style="zoom:80%;" />

<img src="images/README/image-20200606094629680.png" alt="image-20200606094629680" style="zoom:80%;" />



### 간략한 코드리뷰

#### 1. 병렬처리를 위한 init 부분

먼저 Flask WEB server에서는 멀티프로세싱과 큐를 이용하여 영상송출(create_frame)부분과 응급환자 인식(processing_frame)기능이 동시에 처리되도록 구현하였다. 이는 객체인식을 수행하는 부분에서 시스템 성능상의 문제로 지연이 발생하는 것을 줄이기 위함이다.

<img src="images/README/image-20200606094652560.png" alt="image-20200606094652560" style="zoom:80%;" />

#### 2. 영상송출 부분 및 응급상황 인식 부분

1. **create_frame** 함수에서 영상 호출 후 응급환자 인식을 위해 프레임을 Queue에 input한다.

2. **processing_frame** 함수에서 1번과정을 통해 Queue에 저장된 프레임을 호출하고 객체 인식(YOLO)을 이용하여 응급환자 발생여부를 확인한다.

<img src="images/README/image-20200606094756830.png" alt="image-20200606094756830" style="zoom:80%;" />

#### 3. 객체 인식 부분

1.  **YOLO** 함수에서 직접 학습 시킨 모델에 대한 네트워크의 설정 및 weights, config, class name을 정의한다.

2. **processing_frame**에서 받아온 프레임을 네트워크에 입력하고 얻은 결과 정보를 **postprocess** 함수에서 추출한다.

<img src="images/README/image-20200606094818584.png" alt="image-20200606094818584" style="zoom:80%;" />

3.  **posetprocess** 함수에서 YOLO를 통해 인식된 결과를 설정한 값(Confidence threshold, Non-maximum suppression threshold)을 기준으로 분류한다.(쓰러진 사람인지 아닌지 분류)

4.  쓰러진 사람이 발견될 경우 **drawPred** 함수에서 쓰러진 사람에 대한 Bounding box를 표시한다.

<img src="images/README/image-20200606094932039.png" alt="image-20200606094932039" style="zoom:80%;" />

#### 4. WEB 부분

1.  사용자가 웹 호출 시 **templates** 폴더에 저장된 **index.html**를 리턴하도록 설정하였다. index.html을 호출하게 되면 **/view_info1**, **/view_info2**를 자동으로 호출하게 되는데 이는 원본영상과 객체 인식 후의 영상을 송출하게 된다.

   <img src="images/README/image-20200606095016131.png" alt="image-20200606095016131" style="zoom:80%;" />

2.  **generate1/2** 함수에서 실시간으로 처리되는 프레임을 반환하여 웹에서 영상처럼 확인가능하도록 구현하였다.

<img src="images/README/image-20200606095109425.png" alt="image-20200606095109425" style="zoom:80%;" />

3. 실제 사용자가 index.html을 호출하면 다음과 같이 확인하게 된다.

   <img src="images/README/image-20200511105706702.png" alt="image-20200511105706702" style="zoom:80%;" />

#### 5. SMS 전송 부분

응급환자가 발생한 경우 네이버 클라우드 플랫폼의 SMS API를 사용하여 관리자에게 SMS를 전송하도록 구현하였다. 

<img src="images/README/image-20200606095251051.png" alt="image-20200606095251051" style="zoom:80%;" />

실제 결과는 다음과 같다.

<img src="images/README/image-20200511105937624.png" alt="image-20200511105937624" style="zoom:80%;" />

### 시스템 시연

시스템 구현시 동작절차는 우선 쓰러진 사람을 객체인식을 통해 찾고 쓰러진사람이 발견될 경우 10초동안 카운트를 증가시킨다. 10초 동안 계속 발견될 경우 SMS API를 통해 관리자에게 해당 내용을 전파하게 된다.

<img src="images/README/image-20200511110348999.png" alt="image-20200511110348999" style="zoom:80%;" />

<img src="images/README/image-20200511110506078.png" alt="image-20200606095342359" style="zoom:80%;" />