{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:50:56.446740Z",
     "start_time": "2020-01-29T23:50:56.442738Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:57:49.019663Z",
     "start_time": "2020-01-29T23:57:48.223820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,737\n",
      "Trainable params: 12,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "inputs:  ['dense_29_input']\n",
      "outputs:  ['dense_33/Sigmoid']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]], 'float32')\n",
    "Y = np.array([[0], [1], [1], [0]], 'float32')\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation ='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X, Y, batch_size=1, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# inputs:  ['dense_input']\n",
    "print('inputs: ', [input.op.name for input in model.inputs])\n",
    "\n",
    "# outputs:  ['dense_4/Sigmoid']\n",
    "print('outputs: ', [output.op.name for output in model.outputs])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save('xor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:57:54.632129Z",
     "start_time": "2020-01-29T23:57:54.591063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00150369],\n",
       "       [0.9901722 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[1, 1], [0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:57:57.036162Z",
     "start_time": "2020-01-29T23:57:56.856162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 121 variables.\n",
      "INFO:tensorflow:Converted 121 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./xor.pb'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = ''\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "    \n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, './', 'xor.pbtxt', as_text=True)\n",
    "tf.train.write_graph(frozen_graph, './', 'xor.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:57:38.256040Z",
     "start_time": "2020-01-29T23:57:38.239045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_25/MatMul', 'dense_25/Relu', 'dense_26/MatMul', 'dense_26/Relu', 'dense_27/MatMul', 'dense_27/Relu', 'dense_28/MatMul', 'dense_28/Relu', 'output/MatMul', 'output/Sigmoid']\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "net = cv2.dnn.readNetFromTensorflow('xor.pb')\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T23:57:00.157627Z",
     "start_time": "2020-01-29T23:57:00.151632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02006051]\n",
      " [0.98854405]\n",
      " [0.99191636]\n",
      " [0.00580151]]\n",
      "[[0.02006051]\n",
      " [0.98854405]\n",
      " [0.99191636]\n",
      " [0.00580151]]\n"
     ]
    }
   ],
   "source": [
    "net.setInput(np.array([[0, 0], [0,1], [1,0], [1,1]]))\n",
    "out = net.forward(outputName='dense_24/Sigmoid')\n",
    "print(out)\n",
    "\n",
    "out = net.forward()\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist 로딩\n",
    "- 내가 keras로 pb로 변환해서 동작하는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.2631 - accuracy: 0.9198 - val_loss: 0.0574 - val_accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21679198dd8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.05740329907569103\n",
      "Test accuracy: 0.9829000234603882\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: out\\assets\n"
     ]
    }
   ],
   "source": [
    "# 일단 반드시 이왈같이 해야함\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "model3 = tf.keras.models.load_model(\"mnist.h5\", compile=False)\n",
    "model3.save(\"out\", save_format='tf') # 폴더명임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'save_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-3eb9fccfc8d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist2.pb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'save_format'"
     ]
    }
   ],
   "source": [
    "#model.save(\"mnist2.pb\", save_format='tf')\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "model3 = keras.models.load_model(\"mnist.h5\", compile=False)\n",
    "\n",
    "export_path = 'saved_model.pb가 저장될 디렉토리'\n",
    "model3.save(\"\", save_format='tf')\n",
    "\n",
    "load_model 을 할 때 두 번째 인자에 compile 옵션을 안 넣으면, 내 모델이 컴파일이 안 되어 있다고 오류가 뜬다. 구글링해보니 컴파일이란 거는 학습할 준비 어쩌고 하던데 하여튼 학습을 하지 않고 추론하는 데에만 모델을 사용할 거라면, compile=False 옵션을 넣어주면 된다고 해서 넣었음.\n",
    "\n",
    "​\n",
    "\n",
    "그리고 export_path는 파일이 저장될 디렉토리를 지정해주어야 하는데, 디렉토리 안이 비어있어야 한다.\n",
    "\n",
    "그리하여 이것을 실행하면 !!\n",
    "[출처] Keras 모델을 TensorFlow Lite로 변환하기 (h5 -> pb -> tflite)|작성자 aainy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 397 variables.\n",
      "INFO:tensorflow:Converted 397 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./mnist.pb'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "#tf.train.write_graph(frozen_graph, './', 'xor.pbtxt', as_text=True)\n",
    "tf.train.write_graph(frozen_graph, './', 'mnist.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.core import framework\n",
    "\n",
    "def find_all_nodes(graph_def, **kwargs):\n",
    "    for node in graph_def.node:\n",
    "        for key, value in kwargs.items():\n",
    "            if getattr(node, key) != value:\n",
    "                break\n",
    "        else:\n",
    "            yield node\n",
    "    raise StopIteration\n",
    "\n",
    "\n",
    "def find_node(graph_def, **kwargs):\n",
    "    try:\n",
    "        return next(find_all_nodes(graph_def, **kwargs))\n",
    "    except StopIteration:\n",
    "        raise ValueError(\n",
    "            'no node with attributes: {}'.format(\n",
    "                ', '.join(\"'{}': {}\".format(k, v) for k, v in kwargs.items())))\n",
    "\n",
    "\n",
    "def walk_node_ancestors(graph_def, node_def, exclude=set()):\n",
    "    openlist = list(node_def.input)\n",
    "    closelist = set()\n",
    "    while openlist:\n",
    "        name = openlist.pop()\n",
    "        if name not in exclude:\n",
    "            node = find_node(graph_def, name=name)\n",
    "            openlist += list(node.input)\n",
    "            closelist.add(name)\n",
    "    return closelist\n",
    "\n",
    "\n",
    "def remove_nodes_by_name(graph_def, node_names):\n",
    "    for i in reversed(range(len(graph_def.node))):\n",
    "        if graph_def.node[i].name in node_names:\n",
    "            del graph_def.node[i]\n",
    "\n",
    "\n",
    "def make_shape_node_const(node_def, tensor_values):\n",
    "    node_def.op = 'Const'\n",
    "    node_def.ClearField('input')\n",
    "    node_def.attr.clear()\n",
    "    node_def.attr['dtype'].type = framework.types_pb2.DT_INT32\n",
    "    tensor = node_def.attr['value'].tensor\n",
    "    tensor.dtype = framework.types_pb2.DT_INT32\n",
    "    tensor.tensor_shape.dim.add()\n",
    "    tensor.tensor_shape.dim[0].size = len(tensor_values)\n",
    "    for value in tensor_values:\n",
    "        tensor.tensor_content += value.to_bytes(4, 'little')\n",
    "    output_shape = node_def.attr['_output_shapes']\n",
    "    output_shape.list.shape.add()\n",
    "    output_shape.list.shape[0].dim.add()\n",
    "    output_shape.list.shape[0].dim[0].size = len(tensor_values)\n",
    "\n",
    "\n",
    "def make_cv2_compatible(graph_def):\n",
    "    # A reshape node needs a shape node as its second input to know how it\n",
    "    # should reshape its input tensor.\n",
    "    # When exporting a model using Keras, this shape node is computed\n",
    "    # dynamically using `Shape`, `StridedSlice` and `Pack` operators.\n",
    "    # Unfortunately those operators are not supported yet by the OpenCV API.\n",
    "    # The goal here is to remove all those unsupported nodes and hard-code the\n",
    "    # shape layer as a const tensor instead.\n",
    "    for reshape_node in find_all_nodes(graph_def, op='Reshape'):\n",
    "\n",
    "        # Get a reference to the shape node\n",
    "        shape_node = find_node(graph_def, name=reshape_node.input[1])\n",
    "\n",
    "        # Find and remove all unsupported nodes\n",
    "        garbage_nodes = walk_node_ancestors(graph_def, shape_node,\n",
    "                                            exclude=[reshape_node.input[0]])\n",
    "        remove_nodes_by_name(graph_def, garbage_nodes)\n",
    "\n",
    "        # Infer the shape tensor from the reshape output tensor shape\n",
    "        if not '_output_shapes' in reshape_node.attr:\n",
    "            raise AttributeError(\n",
    "                'cannot infer the shape node value from the reshape node. '\n",
    "                'Please set the `add_shapes` argument to `True` when calling '\n",
    "                'the `Session.graph.as_graph_def` method.')\n",
    "        output_shape = reshape_node.attr['_output_shapes'].list.shape[0]\n",
    "        output_shape = [dim.size for dim in output_shape.dim]\n",
    "\n",
    "        # Hard-code the inferred shape in the shape node\n",
    "        make_shape_node_const(shape_node, output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-fd3fc9516155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_variables_to_constants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmake_cv2_compatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Print the graph nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-03460a74436e>\u001b[0m in \u001b[0;36mmake_cv2_compatible\u001b[1;34m(graph_def)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Hard-code the inferred shape in the shape node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mmake_shape_node_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-127-03460a74436e>\u001b[0m in \u001b[0;36mmake_shape_node_const\u001b[1;34m(node_def, tensor_values)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_content\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'little'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_output_shapes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "sess = K.get_session()\n",
    "graph_def = sess.graph.as_graph_def(add_shapes=True)\n",
    "graph_def = tf.graph_util.convert_variables_to_constants(sess, graph_def, [model.output.name.split(':')[0]])\n",
    "make_cv2_compatible(graph_def)\n",
    "\n",
    "# Print the graph nodes\n",
    "print('\\n'.join(node.name for node in graph_def.node))\n",
    "\n",
    "# Save the graph as a binary protobuf2 file\n",
    "tf.train.write_graph(graph_def, '', 'mnist.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\tensorflow\\tf_io.cpp:42: error: (-2:Unspecified error) FAILED: ReadProtoFromBinaryFile(param_file, param). Failed to parse GraphDef file: saved_model.pb in function 'cv::dnn::ReadTFNetParamsFromBinaryFileOrDie'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e26c50dec49a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#net = cv2.dnn.readNetFromTensorflow('MINIST_CNN_frozen_graph.pb')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromTensorflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_model.pb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlayersNames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayersNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\tensorflow\\tf_io.cpp:42: error: (-2:Unspecified error) FAILED: ReadProtoFromBinaryFile(param_file, param). Failed to parse GraphDef file: saved_model.pb in function 'cv::dnn::ReadTFNetParamsFromBinaryFileOrDie'\n"
     ]
    }
   ],
   "source": [
    "#net = cv2.dnn.readNetFromTensorflow('MINIST_CNN_frozen_graph.pb')\n",
    "import cv2 as cv2\n",
    "net = cv2.dnn.readNetFromTensorflow('saved_model.pb')\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((28,28), dtype=np.uint8)  #  이미지 일때는 uint8이어야 한다. 그래야 에러 없다.\n",
    "blob = cv2.dnn.blobFromImage(img) #  학습할때 256으로 여기서 나누어야 한다.\n",
    "print(blob.shape)\n",
    "print(blob.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 784 into shape (0,1,28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-8bd2c27f65a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#blob = blob.reshape(1, 28,28,1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 784 into shape (0,1,28,28)"
     ]
    }
   ],
   "source": [
    "#N: number of images in the batch\n",
    "#H: height of the image\n",
    "#W: width of the image\n",
    "#C: number of channels of the image\n",
    "    \n",
    "#blob = blob.reshape(1, 28,28,1)  #NCHW\n",
    "blob = blob.reshape(1, 1, 28,28)  #NHWC\n",
    "net.setInput(blob)\n",
    "out = net.forward()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 276,138\n",
      "Trainable params: 276,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('cifar10.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 272 variables.\n",
      "INFO:tensorflow:Converted 272 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./cifar.pbtxt'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, './', 'cifar.pb', as_text=False)\n",
    "tf.train.write_graph(frozen_graph, './', 'cifar.pbtxt', as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv2d_1_2/convolution', 'conv2d_1_2/Relu', 'conv2d_2_2/convolution', 'conv2d_2_2/Relu', 'max_pooling2d_1_2/MaxPool', 'conv2d_3_2/convolution', 'conv2d_3_2/Relu', 'conv2d_4_2/convolution', 'conv2d_4_2/Relu', 'max_pooling2d_2_2/MaxPool', 'conv2d_5_2/convolution', 'conv2d_5_2/Relu', 'conv2d_6_2/convolution', 'conv2d_6_2/Relu', 'max_pooling2d_3_2/MaxPool', 'flatten_1_2/Shape', 'flatten_1_2/strided_slice', 'flatten_1_2/Prod', 'flatten_1_2/stack_6803', 'flatten_1_2/Reshape', 'dense_1_2/MatMul', 'dense_1_2/Relu', 'dense_2_2/MatMul', 'dense_2_2/Softmax']\n"
     ]
    }
   ],
   "source": [
    "net = cv2.dnn.readNetFromTensorflow('cifar.pb')\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "[[[0.61960784 0.43921569 0.19215686]\n",
      "  [0.62352941 0.43529412 0.18431373]\n",
      "  [0.64705882 0.45490196 0.2       ]\n",
      "  ...\n",
      "  [0.5372549  0.37254902 0.14117647]\n",
      "  [0.49411765 0.35686275 0.14117647]\n",
      "  [0.45490196 0.33333333 0.12941176]]\n",
      "\n",
      " [[0.59607843 0.43921569 0.2       ]\n",
      "  [0.59215686 0.43137255 0.15686275]\n",
      "  [0.62352941 0.44705882 0.17647059]\n",
      "  ...\n",
      "  [0.53333333 0.37254902 0.12156863]\n",
      "  [0.49019608 0.35686275 0.1254902 ]\n",
      "  [0.46666667 0.34509804 0.13333333]]\n",
      "\n",
      " [[0.59215686 0.43137255 0.18431373]\n",
      "  [0.59215686 0.42745098 0.12941176]\n",
      "  [0.61960784 0.43529412 0.14117647]\n",
      "  ...\n",
      "  [0.54509804 0.38431373 0.13333333]\n",
      "  [0.50980392 0.37254902 0.13333333]\n",
      "  [0.47058824 0.34901961 0.12941176]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.26666667 0.48627451 0.69411765]\n",
      "  [0.16470588 0.39215686 0.58039216]\n",
      "  [0.12156863 0.34509804 0.5372549 ]\n",
      "  ...\n",
      "  [0.14901961 0.38039216 0.57254902]\n",
      "  [0.05098039 0.25098039 0.42352941]\n",
      "  [0.15686275 0.33333333 0.49803922]]\n",
      "\n",
      " [[0.23921569 0.45490196 0.65882353]\n",
      "  [0.19215686 0.4        0.58039216]\n",
      "  [0.1372549  0.33333333 0.51764706]\n",
      "  ...\n",
      "  [0.10196078 0.32156863 0.50980392]\n",
      "  [0.11372549 0.32156863 0.49411765]\n",
      "  [0.07843137 0.25098039 0.41960784]]\n",
      "\n",
      " [[0.21176471 0.41960784 0.62745098]\n",
      "  [0.21960784 0.41176471 0.58431373]\n",
      "  [0.17647059 0.34901961 0.51764706]\n",
      "  ...\n",
      "  [0.09411765 0.30196078 0.48627451]\n",
      "  [0.13333333 0.32941176 0.50588235]\n",
      "  [0.08235294 0.2627451  0.43137255]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = test_images[0,:,:,:] / 255\n",
    "print(img.shape)\n",
    "\n",
    "plt.imshow(img)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model.predict(img.reshape(1,32,32,3))\n",
    "\n",
    "np.argmax(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "img = (img*255).astype(dtype='uint8')\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255) #  학습할때 256으로 나누었음.\n",
    "print(blob.shape)\n",
    "print(blob.dtype)\n",
    "\n",
    "#why ? 보통 1,32,32,3으로 넘겨야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\dnn.cpp:525: error: (-2:Unspecified error) Can't create layer \"flatten_1_2/Shape\" of type \"Shape\" in function 'cv::dnn::dnn4_v20190902::LayerData::getLayerInstance'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-f34891369778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\dnn.cpp:525: error: (-2:Unspecified error) Can't create layer \"flatten_1_2/Shape\" of type \"Shape\" in function 'cv::dnn::dnn4_v20190902::LayerData::getLayerInstance'\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "net.setInput(blob)\n",
    "out = net.forward()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "  \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "  all its variables into constant \n",
    "  Args:\n",
    "      model_dir: the root folder containing the checkpoint state file\n",
    "      output_node_names: a string, containing all the output node's names, \n",
    "                          comma separated\n",
    "                        \"\"\"\n",
    "  if not tf.gfile.Exists(model_dir):\n",
    "    raise AssertionError(\n",
    "      \"Export directory doesn't exists. Please specify an export \"\n",
    "      \"directory: %s\" % model_dir)\n",
    "\n",
    "  if not output_node_names:\n",
    "    print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "    return -1\n",
    "\n",
    "  # We retrieve our checkpoint fullpath\n",
    "  checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "  input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "  # We precise the file fullname of our freezed graph\n",
    "  absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "  output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "  print(output_graph)\n",
    "\n",
    "  # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "  clear_devices = True\n",
    "\n",
    "  # We start a session using a temporary fresh Graph\n",
    "  with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # We import the meta graph in the current default Graph\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "    # We restore the weights\n",
    "    saver.restore(sess, input_checkpoint)\n",
    "\n",
    "    # We use a built-in TF helper to export variables to constants\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "      sess, # The session is used to retrieve the weights\n",
    "      tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "      output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "    ) \n",
    "\n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "  return output_graph_def\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './k', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026EEC757D68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "./k/keras/frozen_model.pb\n",
      "INFO:tensorflow:Restoring parameters from ./k/keras/keras_model.ckpt\n",
      "INFO:tensorflow:Froze 12 variables.\n",
      "INFO:tensorflow:Converted 12 variables to const ops.\n",
      "56 ops in the final graph.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_model = tf.keras.estimator.model_to_estimator(keras_model = model, model_dir = './k')\n",
    "#k 폴더에 keras폴더에 chekc point 파일 저장함\n",
    "# out이름 어덯게 알지?\n",
    "freeze_graph('./k/keras/',\"activation_3/Sigmoid\")\n",
    "\n",
    "#./k/keras/frozen_model.pb에 생성\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,993\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'activation_3'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[7].name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
