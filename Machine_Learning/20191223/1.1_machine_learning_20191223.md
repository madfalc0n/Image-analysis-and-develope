

## 1. K-NN 알고리즘
 - 패턴 인식에서, k-최근접 이웃 알고리즘(또는 줄여서 k-NN)은 분류나 회귀에 사용되는 비모수 방식이다.[1] 두 경우 모두 입력이 특징 공간 내 k개의 가장 가까운 훈련 데이터로 구성되어 있다. 
 - 출력은 k-NN이 분류로 사용되었는지 또는 회귀로 사용되었는지에 따라 다르다.
     - k-NN 분류에서 출력은 소속된 항목이다. 객체는 k개의 최근접 이웃 사이에서 가장 공통적인 항목에 할당되는 객체로 과반수 의결에 의해 분류된다(k는 양의 정수이며 통상적으로 작은 수). 만약 k = 1 이라면 객체는 단순히 하나의 최근접 이웃의 항목에 할당된다.
     - k-NN 회귀에서 출력은 객체의 특성 값이다. 이 값은 k개의 최근접 이웃이 가진 값의 평균이다.
 - k-NN 알고리즘의 정확성은 잡음 또는 무관한 특징이 존재하거나 특징 크기가 중요성과 일치하지 않을 경우 상당히 감소한다.
 
 
## 2. SVM(support vector machine) 알고리즘 
 - P.
 - 분류의 경계를 최소화 하는 알고리즘,
 - 마진을 최대화 한다. 
 - 참고: https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/
 
 
 
## 3. Decision tree
 - P.117
 - 훈련 데이터에 있는 특성을 기반으로 샘플의 클래스 레이블을 추정할 수 있는 일련의 질문을 학습
 - 정보 이득이 최대가 되는 특성으로 데이터를 나눔
 - 노드가 깊을 수록 오버피팅이 된다.
 
 
 
## 4. K-means 알고리즘
 - P.386
 - 비지도학습이다 즉, 예측 변수를 지정할 필요가 없고 필요한 속성을 모두 설명 변수로 활용함
 - 사전에 K 값에 대한 품질 검증이 필요하며 엘보우 방법과 실루엣 그래프 방법이 있다
 - 
 
 
 
 
## 2. Under-fitting 과 Over-fitting 
 - Under-fitting : 많은 공통특성 중 일부 특성만 반영하여, too bias 하게 train 되어 새로운 데이터도 막 예측해버리는 모델
 - Over-fitting : 많은 공통특성 이외에 지염적인 특성까지 반영해, high variance하게 train되어, 새로운 데이터에 대해서는 예측하지 못하는 모델
 
 - Bias : 실제 값에서 멀어진 척도
 - Variance : 예측된 값들이 서로 얼마나 떨어져 있는가
 - 참고:https://nittaku.tistory.com/289